---
title: "Machine Learning Models from Scratch"
author: "Luke Andrade"
format:
  gfm:
    html-math-method: webtex
editor: visual
---

## Packages and Data

```{r}
#| warning: false
#| error: false

library(tidyverse)
library(R6)
library(ISLR2)
df <- Boston
head(Boston)
```

## Simple Linear Regression

#### Creating a Simple Linear Regression Class

We will start by creating a class that has the values $x$, $y$, $\bar{x} = mean(x)$, and $\bar{y} = mean(y)$

```{r}
SLR <- R6Class('SLR',
               list(
                 y = vector(),
                 x = vector(),
                 initialize = function(y, x){
                   stopifnot(is.numeric(y), sum(is.na(y)) == 0)
                   stopifnot(is.numeric(x), sum(is.na(x)) == 0)
                   stopifnot(length(x) == length(y))
                                
                   self$y <- y
                   self$x <- x
                   }
                 ),
               lock_objects = F
               )
SLR
```

First we want to calculate the linear regression model coefficients. The model will follow the form $y = \beta_0 + \beta_1 x$ where

$$\beta_1 = \frac{cov(X,Y)}{var(X)}$$ $$\beta_0 = \bar{y} - \beta_1 \bar{x}$$

```{r}
SLR$set('public', 'calc_coeffs',
        function(){
          self$y_bar <- mean(self$y)
          self$x_bar <- mean(self$x)
          self$b1 = cov(self$x, self$y) / var(self$x)
          self$b0 = self$y_bar - self$b1 * self$x_bar
          invisible(self)
        }
        )
```

Next we create a method to make predictions and add the residuals to the object where residuals are $e_i = y_i - \hat{y}$ and $\hat{y}$ are the fitted values. We will also create a plot that includes the training data and predicted points.

```{r}
SLR$set('public', 'predict',
        function(new_data, plot = T){
          preds <- self$b0 + self$b1 * new_data
          
          if(plot == T){
            self$graph <- ggplot(mapping = aes(self$x, self$y)) +
              geom_point() +
              geom_point(aes(new_data, preds), color = 'blue') +
              labs(x = 'X', y = 'Y', title = 'Scatterplot with Predictions (blue)')
          }
          
          return(preds)
        })

SLR$set('public', 'calc_resids',
        function(){
          self$fitted <- self$predict(self$x, F)
          self$residuals <- self$y - self$fitted
          invisible(self)
        })
```

Next we want to calculate the following sum of squares $$SS_X = \sum_{i = 1}^n (x_i - \bar x)^2$$ $$SS_Y = \sum_{i = 1}^n (y_i - \bar y)^2$$ $$SS_{residuals} = \sum_{i = 1}^n e_i^2$$

```{r}
SLR$set('public', 'calc_ss',
        function(){
          self$ssx <- sum((self$x - self$x_bar)^2)
          self$ssy <- sum((self$y - self$y_bar)^2)
          self$sse <- sum(self$residuals^2)
          invisible(self)
        })
```

Next we want to create a table that shows all of our coefficients, standard errors, t-statistics, and p-values.

$$\sigma = \sqrt\frac{SS_{residuals}}{n-2}$$ $$SE_{\beta_0} = \sigma\sqrt{\frac{1}{n} + \frac{\bar x^2}{SS_X}}$$ $$SE_{\beta_1} = \frac{\sigma}{\sqrt{SS_X}}$$ $$SE_{residuals} = \sqrt{\frac{SS_{residuals}}{n-2}}$$

$$t_{\beta_0} = \frac{\beta_0}{SE_{\beta_0}}$$

$$t_{\beta_1} = \frac{\beta_1}{SE_{\beta_1}}$$

```{r}
SLR$set('public', 'create_coeff_table',
        function(){
          sigma <- sqrt(self$sse / (length(self$residuals) - 2))
          se_b0 <- sigma*sqrt(1 / length(self$residuals) + self$x_bar^2 / self$ssx)
          se_b1 <- sigma / sqrt(self$ssx)
          se_r <- sqrt(self$sse / (length(self$residuals) - 2))
          t_b0 <- self$b0 / se_b0
          t_b1 <- self$b1 / se_b1
          pval_b0 <- 1 - pt(abs(t_b0), length(self$residuals) - 2)
          pval_b1 <- 1 - pt(abs(t_b1), length(self$residuals) - 2)
          
          self$coeff_table <- data.frame(Parameter = c('Intercept', 'X Variable'),
                                         Coefficient = c(self$b0, self$b1),
                                         SE = c(se_b0, se_b1),
                                         t_stat = c(t_b0, t_b1),
                                         p_val = c(pval_b0, pval_b1))
          invisible(self)
        })
```

Next we will calculate the value $R^2$.

$$R^2 = \frac{SS_Y - SS_{residuals}}{SS_Y}$$

```{r}
SLR$set('public', 'calc_r2',
        function(){
          self$r2 <- (self$ssy - self$sse) / self$ssy
          invisible(self)
        })
```

Now we want to print out all of the necessary information when the model is viewed.

```{r}
SLR$set('public', 'print',
        function(){
          cat('Model: \n')
          cat('  Coeff  Estimate  SE  t-stat  p-val \n')
          cat(' ', self$coeff_table[[1,1]], round(self$coeff_table[[1,2]],3), round(self$coeff_table[[1,3]],3), round(self$coeff_table[[1,4]],3), round(self$coeff_table[[1,5]],3), '\n')
          cat(' ', self$coeff_table[[2,1]], round(self$coeff_table[[2,2]],3), round(self$coeff_table[[2,3]],3), round(self$coeff_table[[2,4]],3), round(self$coeff_table[[2,5]],3), '\n')
          cat('\n R-squared: ', round(self$r2,4))
        })
```

#### Creating a Function to Create the Model

```{r}
SimpleLinearRegression <- function(y, x){
  mod <- SLR$new(y, x)$calc_coeffs()$calc_resids()$calc_ss()$create_coeff_table()$calc_r2()
  return(mod)
}
```

Now we will test it against the lm() function using the Boston housing dataset.

```{r}
mod1 <- SimpleLinearRegression(df$medv, df$lstat)
mod1
```

```{r}
mod1.1 <- lm(medv ~ lstat, data = df)
summary(mod1.1)
```

We can easily see that the results are the same so we have a success!

Let's check if predictions work.

```{r}
new_data <- runif(n = 50, min = min(df$lstat), max = max(df$lstat))
preds <- mod1$predict(new_data)
mod1$graph
```
